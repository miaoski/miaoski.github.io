apiVersion: v1
kind: Pod
metadata:
  name: llama-cpp-gpu-pod
  labels:
    purpose: demo-llama-cpp-amdgpu
spec:
  containers:
    - name: llama-cpp-gpu-container
      image: miaoski/llama-amdgpu:latest
      workingDir: /root/LeftoverLocalsRelease
      env:
      - name: HIP_VISIBLE_DEVICES
        value: "0" # # 0,1,2,...,n for running on GPU and select the GPUs, -1 for running on CPU
      command: ["/root/run.sh"]
      resources:
        limits:
          amd.com/gpu: 1 # requesting a GPU
